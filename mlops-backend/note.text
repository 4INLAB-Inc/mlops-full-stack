add griffe==0.49.0 to requirements.txt in service/prefect and service/prefect_worker

1. Open the Jupyter lab on port 8888 http://localhost:8888/lab
2. Go to the workspace directory cd ~/workspace/
3. Activate the conda environment (the name is configurable in docker-compose.yml) conda activate mlops-4inlab
4. Run:  python run_flow.py --config configs/full_flow_config.yaml

5. If you face ImportError: /lib/aarch64-linux-gnu/libGLdispatch.so.0: cannot allocate memory in static TLS block error, 
try "export LD_PRELOAD=/lib/aarch64-linux-gnu/libGLdispatch.so.0" then rerun your script.


# Run flow with selected config and model_type
python run_flow.py --config configs/full_flow_config.yaml --model_type GRU
python run_flow.py --config configs/data_flow_config.yaml
python run_flow.py --config configs/eval_flow_config.yaml
python run_flow.py --config configs/eval_flow_config.yaml

python run_flow.py --config configs/full_flow_config.yaml --data_flow 1 --train_flow 1 --eval_flow 1 --deploy_flow 1 --model_type Conv1D_BiLSTM --epochs 100
python run_flow.py --config configs/full_flow_config.yaml --deploy_flow 1
model_type: LSTM, GRU, BiLSTM, Conv1D_BiLSTM


#Run api_flow in jupyer container:
PS C:\Users\20240805> docker exec -it jupyter bash
(base) ariya@038ce05b2bfb:~$ cd workspace 
(base) ariya@038ce05b2bfb:~/workspace$ gunicorn -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:9992 api_flow:app --timeout 1200
cd /home/ariya/workspace/ && nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu --format=csv,noheader,nounits





